{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "BENCHMARKS = ['hh_bench', 'ihap_bench', 'mt_bench', 'vicuna_bench']\n",
    "MODEL_PARALLEL = {\n",
    "    # 'Locutusque/TinyMistral-248M-v2.5': 12, #done\n",
    "    # 'TinyLlama/TinyLlama-1.1B-Chat-v1.0': 10, #done\n",
    "    # 'Writer/palmyra-small': 10, #done\n",
    "    # 'Writer/palmyra-3B': 4, #done\n",
    "    # 'lmsys/vicuna-7b-v1.5': 2, #done\n",
    "    # 'lmsys/vicuna-13b-v1.5': 1, #done\n",
    "    # 'mistralai/Mistral-7B-Instruct-v0.2': 2, #done\n",
    "    # 'microsoft/phi-2': 8, #done\n",
    "    # 'microsoft/phi-1_5': 10, #done\n",
    "    # 'abacaj/phi-2-super': 6, #done\n",
    "    # 'AbacusResearch/RasGulla1-7b': 2 #done\n",
    "    # 'Jiayi-Pan/Tiny-Vicuna-1B': 10,\n",
    "    # 'pansophic/rocket-3B': 4,\n",
    "}\n",
    "\n",
    "for model in MODEL_PARALLEL:\n",
    "    name = model.split('/')[1]\n",
    "    gpus = MODEL_PARALLEL[model]\n",
    "    for bench in BENCHMARKS:\n",
    "        subprocess.run(f\"python3 scripts/gen_model_answer.py --model-path {model} --model-id {name} --num-gpus-total {gpus} --bench-name {bench}\".split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os \n",
    "\n",
    "BENCHMARKS = ['hh_bench', 'ihap_bench', 'mt_bench', 'vicuna_bench']\n",
    "\n",
    "for bench in BENCHMARKS:\n",
    "    names = os.listdir(f'data/{bench}/model_answer/')\n",
    "    names = [name.split('.jsonl')[0] for name in names]\n",
    "    names = ' '.join(names)\n",
    "    subprocess.run(f\"python3 scripts/gen_judgement.py --model-list {names} --parallel 30 --bench-name {bench} --judge-model gpt-4-0125-preview\".split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: single\n",
      "Input file: data/hh_bench/model_judgment/gpt-4-0125-preview_single.jsonl\n",
      "                          score  (timing, mean)  (timing, percentile_10)  (timing, percentile_90)\n",
      "model                                                                                            \n",
      "Mistral-7B-Instruct-v0.2   8.04      226.331116               137.213610               321.280518\n",
      "RasGulla1-7b               8.27      439.810492               193.328913               662.938596\n",
      "TinyLlama-1.1B-Chat-v1.0   5.23      133.823367                55.771897               198.554997\n",
      "TinyMistral-248M-v2.5      1.00      107.294026               101.599922               112.630434\n",
      "palmyra-3B                 2.86     1242.736198              1230.625022              1282.354318\n",
      "palmyra-small              1.18       62.461301                24.913231                76.375889\n",
      "phi-1_5                    4.58      589.768445               570.585325               607.298617\n",
      "phi-2                      6.32      216.892511                54.531658               476.875466\n",
      "phi-2-super                5.89      738.240612               691.915606               830.038252\n",
      "vicuna-13b-v1.5            7.51      235.594860                66.138441               442.573528\n",
      "vicuna-7b-v1.5             7.18      148.251433                61.043772               225.902019\n",
      "Mode: single\n",
      "Input file: data/ihap_bench/model_judgment/gpt-4-0125-preview_single.jsonl\n",
      "                          score  (timing, mean)  (timing, percentile_10)  (timing, percentile_90)\n",
      "model                                                                                            \n",
      "Mistral-7B-Instruct-v0.2   8.31      331.010019               189.880810               467.413416\n",
      "RasGulla1-7b               8.35      407.738971               199.082198               664.350423\n",
      "TinyLlama-1.1B-Chat-v1.0   6.21      169.468690                96.897633               252.747914\n",
      "TinyMistral-248M-v2.5      1.00      107.548492               100.878076               113.297252\n",
      "palmyra-3B                 3.47     1223.424705              1210.848665              1279.030819\n",
      "palmyra-small              1.23       69.513934                66.674960                75.120946\n",
      "phi-1_5                    5.89      594.374689               564.398390               613.197150\n",
      "phi-2                      7.45      255.772412                77.355375               508.944478\n",
      "phi-2-super                7.26      738.389320               696.393866               821.485701\n",
      "vicuna-13b-v1.5            7.46      337.683239               148.802794               468.080559\n",
      "vicuna-7b-v1.5             7.41      158.804735                94.071468               225.645243\n",
      "Mode: single\n",
      "Input file: data/mt_bench/model_judgment/gpt-4-0125-preview_single.jsonl\n",
      "                           score  (timing, mean)  (timing, percentile_10)  (timing, percentile_90)\n",
      "model                                                                                             \n",
      "Mistral-7B-Instruct-v0.2  6.7875      183.197434                38.855226               342.930172\n",
      "RasGulla1-7b              6.8750      240.648443                50.461483               557.234767\n",
      "TinyLlama-1.1B-Chat-v1.0  3.5375      155.264986                26.364214               299.127541\n",
      "TinyMistral-248M-v2.5     1.0000      131.813274               101.498077               189.464401\n",
      "palmyra-3B                5.6250             NaN                      NaN                      NaN\n",
      "palmyra-small             3.2250             NaN                      NaN                      NaN\n",
      "phi-1_5                   3.8750      648.925948               546.574809               853.988803\n",
      "phi-2                     5.8125      344.308501                63.975248               926.657106\n",
      "phi-2-super               5.4750      811.298088               685.616640               973.705260\n",
      "vicuna-13b-v1.5           5.8625      409.044941                41.704788               857.824201\n",
      "vicuna-7b-v1.5            5.8125      136.227863                28.484685               276.483044\n",
      "rocket-3B                    NaN      196.997333                60.801514               442.547597\n",
      "Tiny-Vicuna-1B               NaN       94.202908                 9.701539               227.237282\n",
      "Mode: single\n",
      "Input file: data/vicuna_bench/model_judgment/gpt-4-0125-preview_single.jsonl\n",
      "                           score  (timing, mean)  (timing, percentile_10)  (timing, percentile_90)\n",
      "model                                                                                             \n",
      "Mistral-7B-Instruct-v0.2  8.1000      243.311656               155.604569               358.997584\n",
      "RasGulla1-7b              7.8750      307.601593               149.561674               648.443349\n",
      "TinyLlama-1.1B-Chat-v1.0  5.4875      145.940792                54.074841               239.125828\n",
      "TinyMistral-248M-v2.5     1.0000      114.615406               107.200628               121.581196\n",
      "palmyra-3B                2.6250     1548.601994              1252.481424              1701.073156\n",
      "palmyra-small             1.1000       64.955747                40.152384                75.145442\n",
      "phi-1_5                   5.4250      606.533207               596.821296               617.855565\n",
      "phi-2                     7.0375      192.169180                74.889902               364.253381\n",
      "phi-2-super               6.6625      727.821374               694.731901               822.486163\n",
      "vicuna-13b-v1.5           6.9125      271.979689               118.961712               439.354835\n",
      "vicuna-7b-v1.5            6.3625      169.982997                79.802274               271.854781\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os \n",
    "\n",
    "BENCHMARKS = ['hh_bench', 'ihap_bench', 'mt_bench', 'vicuna_bench']\n",
    "\n",
    "for bench in BENCHMARKS:\n",
    "    names = os.listdir(f'data/{bench}/model_answer/')\n",
    "    names = [name.split('.jsonl')[0] for name in names]\n",
    "    names = ' '.join(names)\n",
    "    subprocess.run(f\"python3 -W ignore scripts/show_result.py --model-list {names} --bench-name {bench}\".split(\" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
